# ğŸš€ End-to-End ETL Pipeline with Azure Databricks

Welcome to my data engineering project showcasing a complete, production-grade ETL pipeline using **Azure Databricks** and **Delta Live Tables**, built on the **Medallion Architecture**.

![Architecture Diagram](Documents/bricksproject.jpg)

## ğŸ“Œ Overview

This project demonstrates how to build scalable, modular, and cloud-native data pipelines using modern tools and best practices. It covers everything from raw data ingestion to analytical reporting.

## ğŸ§± Architecture Highlights

- ğŸ¥‰ **Bronze Layer**: Raw data ingestion using Spark Autoloader  
- ğŸ¥ˆ **Silver Layer**: Data transformation and enrichment with PySpark  
- ğŸ¥‡ **Gold Layer**: Star schema design for analytics, powered by Delta Live Tables  
- ğŸ”„ **ETL Orchestration**: Managed with Databricks Workflows  
- ğŸ” **Security & Governance**: Enforced via Unity Catalog  
- ğŸ“Š **Reporting**: Data served to Power BI and Azure Synapse Analytics

## âš™ï¸ Technologies Used

| Tool/Service              | Role in Pipeline                              |
|--------------------------|-----------------------------------------------|
| **Azure Databricks**     | Unified platform for ETL, analytics, and ML   |
| **Azure Data Lake Gen2** | Scalable storage for raw and processed data   |
| **PySpark**              | Data transformation and enrichment            |
| **Delta Lake**           | ACID-compliant storage with versioning        |
| **Delta Live Tables**    | Declarative ETL pipelines with data quality   |
| **Unity Catalog**        | Data governance and reusable functions        |
| **Databricks Workflows** | Pipeline orchestration and scheduling         |
| **Power BI / Synapse**   | Reporting and data warehousing                |

## ğŸ”§ Key Features

- âœ… Incremental data loading with Spark Streaming and Autoloader  
- ğŸ§¼ Complex data cleaning and enrichment using PySpark  
- ğŸ§© Modular, reusable code with Python OOP in PySpark  
- ğŸ•°ï¸ SCD Type 1 & Type 2 implementation for historical tracking  
- ğŸ“ Star schema design for analytical querying  
- ğŸ”„ End-to-end orchestration with Databricks Workflows  
- ğŸ” Unity Catalog for governance and function reuse  
- ğŸ“Š Reporting integration with Power BI and Synapse



